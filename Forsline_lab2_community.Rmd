---
title: "Lab 2 Community"
author: "Mia Forsline"
date: "2021-01-31"
output: 
  html_document:
    theme: flatly
    code_folding: show
    toc: true
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE, 
                      include = TRUE,
                      quiet = TRUE)
```

# Learning Objectives

**Lab 2a: Cluster**

Use unsupervised classification techniques to:

-   compare species counts between sites using different distance
    metrics:

    -   **Euclidean distance**: based on the Pythagorean Theorem

    -   **Manhattan distance**: integer "around the block" distance

    -   **Bray-Curtis dissimilarity**: sum of lowest counts of shared
        species between sites over the sum of all species

        -   a dissimilarity value of 1 is completely dissimilar, i.e. no
            species shared

        -   a value of 0 is completely identical

-   cluster

    -   *k*-means clustering using `kmeans()` given a pre-assigned
        number of clusters assigns membership centroid based on reducing
        within cluster variation

        -   visualize using Voronoi diagrams

    -   **hierarchical clustering** using a non-specific number of
        clusters

        -   agglomerative hierarchical clustering using `diana()`: good
            for identifying small clusters

        -   divisive hierarchical clustering using `agnes()`: good for
            identifying large clusters

        -   dendrograms to visualize the branching tree

-   ordinate

**Lab 2b: Ordination**

-   ordination orders sites near each other based on similarity

-   ordination is a multivariate analysis technique that collapses many
    dependent axes into fewer dimensions (dimensionality reduction)

    -   **Principal Components Analysis (PCA)**: assumes linear
        relationships between axes

    -   **Non-metric MultiDimensional Scaling (NMDS)**: allows for
        non-linear relationships between axes

        -   uses the `vegan` R package

        -   **unconstrained ordination** on species

        -   overlay with environmental gradients

        -   **constrained ordination** on species and environment using
            **canonical correspondence analysis (CCA)**

# Set Up

-   load packages
-   set seed for reproducible results

```{r}
librarian::shelf(
  cluster, 
  dplyr, 
  DT, 
  factoextra, 
  ggplot2, 
  ggvoronoi, 
  h2o, 
  scales,
  tibble, 
  vegan, 
  vegan3d)

library(tidyverse)

set.seed(42)
```

# Lab 2a: Clustering

-   an unsupervised learning technique that associates similar data
    points with each other

## K-Means Clustering

-   we specify the number of clusters
-   then the algorithm assigns each observation to a cluster based on
    the centroid of each cluster
-   the algorithm iterates between two steps:

1.  reassign data points to the cluster with the nearest centroid
2.  calculate a new centroid for each cluster

-   the algorithm repeats these two steps until the cluster variation is
    minimized
-   cluster variation is calculated using Euclidean distance between
    data points and their respective centroid

## Load and plot the `palmerpenguins` data

```{r}
# load the dataset
library(palmerpenguins)

peng_clean <- penguins %>% 
  mutate(bill_depth_mm = as.numeric(penguins$bill_depth_mm)) %>% 
  drop_na()

# show data table
datatable(peng_clean)
```

Plot the bill length and bill depth naive of species

```{r}
ggplot(
  peng_clean, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point() + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

Plot the bill length and bill depth by species

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  peng_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point() + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

### Cluster `palmerpenguins` using `kmeans()`

-   `k` = number of clusters = 3

-   clean the data to use `kmeans()`

    -   convert columns of interest to numeric
    -   remove NA values

    ```{r}
    k <- 3 

    peng_k <- kmeans(
      peng_clean %>% 
        select(bill_length_mm, bill_depth_mm),
      centers = k)

    # show cluster result
    peng_k
    ```

Compare clusters with species

```{r}
table(peng_k$cluster, peng_clean$species)
```

## **Question:** How many observations could be considered "misclassified" if expecting bill length and flipper length to differentiate between species?

Extract cluster assignment per observation

```{r}
Cluster = factor(peng_k$cluster)

ggplot(peng_clean, aes(x = bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

## Plot Voronoi diagram of clustered `palmerpenguins`

-   assigns points to the cluster based on nearest centroid and helps
    visualize the breaks more clearly
-   define the bounding box for `geom_voronoi()` based on the min and
    max values for bill length and flipper length
-   kmeans cluster again
-   extract cluster assignment per observation
-   extract cluster centers
-   plot points with voronoi diagram showing nearest centroid

```{r}
# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)

box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3 

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

## **Task:** Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.

## Voronoi diagram for k = 2

```{r}
k <- 2 

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

## Voronoi diagram for k = 8

```{r}
k <- 8

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Bill Depth (mm)")
```

## Hierarchical Clustering

-   cluster sites according to species composition using the dune
    dataset from the vegan R package.

## Load `dune` dataset from `vegan` packages

```{r}
data("dune")

if (interactive())
  help(dune)
```

## **Question:** What are the rows and columns composed of in the dune data frame?

The `dune` dataset describes dune meadow vegetation. It has cover class
values of 30 species on 20 sites. The column names are 30 species of
vegetation. The row values are observation numbers for each of the 20
sites.

### Calculate Ecological Distances on `sites`

-   start by using a simpler dataset from Ch. 8 of Kindt and Coe (2005)

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

### Calculate Manhattan distance

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

### Calculate Euclidean distance

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

### Calculate Bray-Curtis dissimilarity

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

## Agglomerative hierarchical clustering on `dune`

-   create a dissimilarity matrix

```{r}
d <- vegdist(dune, method="bray")
dim(d)

as.matrix(d)[1:5, 1:5]
```

## Hierarchical clustering using Complete Linkage and `hclust()`

-   plot dendrogram

```{r}
hc1 <- hclust(d, method = "complete" )

plot(hc1, cex = 0.6, hang = -1)
```

## Compute agglomerative clustering with `agnes()`

-   report the agglomerative coefficient (AC), which measures the amount
    of clustering structure found

    -   in other words AC describes the strength of the clustering
        structure
    -   values closer to 1 suggest a more balanced clustering structure
    -   values closer to 0 suggest less well-formed clusters
    -   however, note that AC tends to increase as *n* increases

-   plot dendrogram

```{r}
hc2 <- agnes(dune, method = "complete")
hc2$ac

plot(hc2, which.plot = 2)
```

## Linkage methods to assess:

-   average
-   single
-   complete
-   ward
-   calculate the agglomerative coefficient for each linkage method

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

## Compute ward linkage glustering with `agnes()`

-   note that the ward method returned the highest agglomerative
    coefficient (AC)
-   report ward agglomerative coefficient
-   plot dendrogram

```{r}
hc3 <- agnes(dune, method = "ward")

hc3$ac

plot(hc3, which.plot = 2)
```

## Divisive hierarchical clustering on `dune` using `diana()`

-   compute divisive coefficienct (DC), which tells us about the amount
    of clustering structure found

    -   a DC closer to 1 suggests stronger group distinctions

-   if you assess the various methods, similar to the prior
    agglomerative example, the ward method returns the highest DC

```{r}
hc4 <- diana(dune)

hc4$dc
```

## Determining optimal clusters

-   dotted lines indicate optimal `k` for that method

-   there may not always be a clear optimal number of clusters

-   plot cluster results using various methods

    -   elbow method
    -   silhouette method
    -   gap statistic

    ```{r}
    p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
      ggtitle("(A) Elbow method")

    p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
      ggtitle("(B) Silhouette method")

    p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
      ggtitle("(C) Gap statistic")

    gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
    ```

## Working with dendrograms

-   the height of the branch between an observation and the clusters of
    observations below them indicate the distance between the
    observation and that cluster it is joined to
-   for example, 15 is closer to 20 than 16 is
-   construct dendorgram for the Ames housing example

```{r}
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

To identify clusters, we can cut the dendrogram using `cutree()` - we
can cut the agglomerative hierarchical clustering model into *k* = 4
clusters

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

We can now plot the full dendrogram and the 4 clusters we just created

```{r}
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

# Lab 2b: Ordination

## Principal Components Analysis (PCA)

## Data set up

-   get the data

-   the `my_basket` data set identifies items and quantities purchased
    for 2,000 transactions from a grocery store

    -   each observation is a single basket of goods that were purchased
        together

-   using this dataset, we will conduct an unsupervised analysis and try
    to use the attributes of each basket to identify items commonly
    purchased together

```{r}
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
my_basket
```

## Start up `h2o`

-   `h2o` provides consistency across dimension reduction methods
-   convert data to an `h2o` object

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

my_basket.h2o <- as.h2o(my_basket)
```

## Run PCA using `h2o.prcomp()`

-   use "GramSVD" method for numeric data ("GLRM" method can be used for
    categorical data)
-   typically, PC1 captures the most variation followed by PC2, PC3,
    etc.

```{r}
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

## Plot PCA

-   assess what items most contribute to PC1

    -   for example, Bulmers (a brand of cider) influences PC1 the most

-   compare PC1 to PC2 to identify distinct and similar groupings of
    items that contribute to them

    -   for example, adult beverages contribute more to PC1 than PC2

    ```{r}
    my_pca@model$eigenvectors %>% 
      as.data.frame() %>% 
      mutate(feature = row.names(.)) %>%
      ggplot(aes(pc1, reorder(feature, pc1))) +
      geom_point() + 
      theme_classic()

    my_pca@model$eigenvectors %>% 
      as.data.frame() %>% 
      mutate(feature = row.names(.)) %>%
      ggplot(aes(pc1, pc2, label = feature)) +
      geom_text() + 
      theme_classic()
    ```

## Eigenvalue criterion

-   the sum of eigenvalues = number of variables entered into the PCA

-   eigenvalues can range from near zero to greater than 1

-   and eigenvalue of 1 means the PC explains about 1 variable's worth
    of the variability

-   each eigenvalue should explain at least one variable's worth of
    variability, so only components with eigenvalues \> 1 should be
    retained

    -   in our case, we should only retain PC1 - PC10

    ```{r}
    # Compute eigenvalues
    eigen <- my_pca@model$importance["Standard deviation", ] %>%
      as.vector() %>%
      .^2
      
    # Sum of all eigenvalues equals number of variables
    sum(eigen)

    # Find PCs where the sum of eigenvalues is greater than or equal to 1
    which(eigen >= 1)
    ```

## Proportion of variance explained (PVE)

-   PVE identifies the optimal number of PCs to keep
-   PVE is based on the total variability we want to account for
-   `h2o.prcomp()` provides the PVE and cumulative variance explained
    (CVE)

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

How many PCs do we need to explain at least 75% of the total
variability?

```{r}
min(which(ve$CVE >= 0.75))
```

\#\#Scree plot criterion - a scree plot shows the eigenvalues or PVE for
each PC - scree plots often start off high and fall rather quickly
because the first few PCs often explain much of the variability, the
next few PCs explain a moderate amount, and the last components explain
very little - this creates an "elbow" curve

```{r}
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002) + 
  theme_classic()
```

## Non-metrid MultiDimensional Scaling (NMDS)

### Unconstrained ordination on species

-   load species (`varespec`) and environmental chemistry (`varechem`)
    data
-   this is vegetation and environment in lichen pastures data from Vare
    et al. (1995)

```{r}
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

## NMDS can be performed using `monoMDS()`

-   `monoMDS()` input = dissimilarities, which we can calculate using
    `vegdist()`
-   the default is Bray-Curtis dissimilarity
-   use `stressplot()` to inspect the mapping of observed community
    dissimilarities onto ordinarition space - note that it is not a
    linear relationship

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

## Plot the results of NMDS using `ordiplot()`

-   shows site scores because dissimilarities input does not have info
    about species

```{r}
ordiplot(vare.mds0, type = "t")
```

## `metaMDS()` is recommended for the NMDS iterative search

-   uses several random starts to select among similar solutions with
    smallest stresses
-   trace = 0 suppresses the long tracing output
-   in this case, we input the original data (rather than the
    dissimilarities)
-   we can plot the output and see the species

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
plot(vare.mds, type = "t")
```

## Overlay with Environment - Environmental interpretation

## Vector fitting

-   fit environmental vectors into ordination

-   the first two columns give direction cosines of the vectors

-   r2 gives the squared correlation coefficient

    -   plot axes should be scaled by the square root of r2 (which
        `plot()` does automatically)

    ```{r}
    ef <- envfit(vare.mds, varechem, permu = 999)
    ef
    ```

## Plot the fitted environmental vectors

-   the fitted vectors are arrows

    -   the arrow points in the direction of the most rapid change in
        that environmental variable (direction of the gradient)
    -   arrow length is proportional to the correlation between
        ordination and environmental variable (strength of the gradient)

-   p.max limits plotting to the most significant variables

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

## Surface fitting

-   vector fitting implies a linear relationship between ordination and
    environment - we only need to know direction and strength

-   `ordisurf()` fits surfaces of environmental variables to ordinations
    by way of generalized additive models (GAM)

    -   if the response is linear and vectors are appropriate, the
        fitted surface is a plane

-   \`envfit()1 uses a formula interface in which Y \~ X

    -   returns a fitted GAM

-   `with()` makes the dataframe visible only to the following command

    -   we may not want to make all variables visible to the R session
        because of confusing/overlapping variable names

-   add fitted surfaces to a plot of fitted vectors with selected
    variables

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

## Constrained ordination on species and environment using `cca()`

-   for a constrained model, it's best to use a model formula (Y \~ X)

    -   in this case, we ordinate species constrained by 3 soil elements

-   the output is similar to unconstrained ordination

-   now we have 3 constrained and 23 unconstrained components

    -   in the prior unconstrained analysis, we had 23 components

-   rank = axes; in other words, we have 3 constrained axes and 23
    unconstrained axes

    -   sometimes there are fewer ranks than axes

    ```{r}
    vare.cca <- cca(varespec ~ Al + P + K, varechem)
    vare.cca
    ```

## Plot the constrained ordination

-   arrows for constraints have similar interpretation as the fitted
    vector arrows

```{r}
plot(vare.cca)
```

## Plot in 3 dimensions using `ordiplot3d()`

```{r}
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```
