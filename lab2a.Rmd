---
title: "Lab 2a: Community - Cluster"
author: "Mia Forsline"
date: "2021-01-31"
output: 
  html_document:
    theme: flatly
    code_folding: show
    toc: true
    toc_float: true
    number_sections: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE, 
                      include = TRUE,
                      quiet = TRUE)
```

# Learning Objectives 

**Lab 2a: Cluster**
Use unsupervised classification techniques to: 
- compare species counts between sites using distance metrics 
  - **Euclidean distance**: based on the Pythagorean Theorem 
  - **Manhattan distance**: integer "around the block" distance
  - **Bray-Curtis dissimilarity**: sum of lowest counts of shared species between sites over the sum of all species
    - a dissimilarity value of 1 is completely dissimilar, i.e. no species shared
    - a value of 0 is completely identical
- cluster
  - *k*-means clustering using `kmeans()` given a pre-assigned number of clusters assigns membership centroid based on reducing within cluster variation
    - visualize using Voronoi diagrams 
  - **hierarchical clustering** using a non-specific number of clusters 
    - agglomerative hierarchical clustering using `diana()`: good for identifying small clusters 
    - divisive hierarchical clustering using `agnes()`: good for identifying large clusters 
    - dendrograms to visualize the branching tree 
    
# Set Up 
- load packages
- set seed for reproducible results 
```{r}
# load R packages
librarian::shelf(
  cluster, 
  dplyr, 
  DT, 
  factoextra, 
  ggplot2, 
  ggvoronoi, 
  h2o, 
  scales,
  tibble, 
  vegan)

library(tidyverse)

set.seed(42)
```

**Lab 2b: Ordination** 
- ordination orders sits near each other based on similarity 
  - multivariate analysis technique to collapse many dependent axes into fewer dimensions (dimensionality reduction) 
- **Principal Components Analysis (PCA)**: assumes linear relationships between axes 
- **Non-metric MultiDimensional Scaling (NMDS)**: allows for non-linear relationships between axes
  - uses the `vegan` R package
  - **unconstrained ordination**
  - overlay with environmental gradients
  - **constrained ordination** on species and environment using **canonical correspondence analysis (CCA)**

# Clustering 
- an unsupervised learning technique that associates similar data points with each other

## K-Means Clustering 
- we specify the number of clusters 
- then the algorithm assigns each observation to a cluster based on the centroid of each cluster 
- the algorithm iterates between two steps: 
1. reassign data points to the cluster with the nearest centroid
2. calculate a new centroid for each cluster 
- the algorithm repeats these two steps until the cluster variation is minimized
- cluster variation is calculated using Euclidean distance between data points and their respective centroid 

## Load and plot the `palmerpenguins` data 
```{r}
# load the dataset
#data("iris")
library(palmerpenguins)

peng_clean <- penguins %>% 
  mutate(flipper_length_mm = as.numeric(penguins$flipper_length_mm)) %>% 
  drop_na()

# show data table
datatable(peng_clean)
```

Plot the bill length and flipper length naive of species 
```{r}
ggplot(
  peng_clean, aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point() + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```
Plot the bill length and flipper length by species 
```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  peng_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point() + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```

### Cluster `palmerpenguins` using `kmeans()`
- `k` = number of clusters = 3
- clean the data to use `kmeans()` 
  - convert columns of interest to numeric 
  - remove NA values
```{r}
k <- 3 

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, flipper_length_mm),
  centers = k)

# show cluster result
peng_k
```

Compare clusters with species 
```{r}
table(peng_k$cluster, peng_clean$species)
```

## **Question:** How many observations could be considered “misclassified” if expecting bill length and flipper length to differentiate between species?

56 observations could be misclassified using bill length and flipper length to differentiate between penguin species. 

Extract cluster assignment per observation 
```{r}
Cluster = factor(peng_k$cluster)

ggplot(peng_clean, aes(x = bill_length_mm, flipper_length_mm, color = Cluster)) +
  geom_point() + 
  legend_pos + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```

## Plot Voronoi diagram of clustered `palmerpenguins` 
- assigns points to the cluster based on nearest centroid and helps visualize the breaks more clearly 
- define the bounding box for `geom_voronoi()` based on the min and max values for bill length and flipper length 
- kmeans cluster again 
- extract cluster assignment per observation
- extract cluster centers
- plot points with voronoi diagram showing nearest centroid
```{r}
box <- tribble(
  ~bill_length_mm, ~flipper_length_mm, ~group,
  30, 170, 1,
  30, 235, 1,
  60, 235, 1,
  60, 170, 1,
  30, 170, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3 

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, flipper_length_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```

# **Task:** Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.

## Voronoi diagram for k = 2
```{r}
k <- 2 

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, flipper_length_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```

## Voronoi diagram for k = 8 
```{r}
k <- 8

peng_k <- kmeans(
  peng_clean %>% 
    select(bill_length_mm, flipper_length_mm),
  centers = k)

Cluster = factor(peng_k$cluster)

ctrs <- as.data.frame(peng_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

ggplot(peng_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black") + 
  theme_classic() + 
  labs(x = "Bill Length (mm)", 
       y = "Flipper Length (mm)")
```


## Hierarchical Clustering 
- cluster sites according to species composition using the dune dataset from the vegan R package.

## Load `dune` dataset from `vegan` packages 
```{r}
data("dune")

if (interactive())
  help(dune)
```

## **Question:** What are the rows and columns composed of in the dune data frame?

The `dune` dataset describes dune meadow vegetation. It has cover class values of 30 species on 20 sites. The column names are 30 species of vegetation. The row values are observation numbers for each of the 20 sites. 

### Calculate Ecological Distances on `sites` 
- start by using a simpler dataset from Ch. 8 of Kindt and Coe (2005) 
```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```
### Calculate Manhattan distance 
```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```
### Calculate Euclidean distance 
```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```
### Calculate Bray-Curtis dissimilarity 
```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

## Agglomerative hierarchical clustering on `dune` 
- create a dissimilarity matrix 
```{r}
d <- vegdist(dune, method="bray")
dim(d)

as.matrix(d)[1:5, 1:5]
```
## Hierarchical clustering using Complete Linkage and `hclust()`
- plot dendrogram
```{r}
hc1 <- hclust(d, method = "complete" )

plot(hc1, cex = 0.6, hang = -1)
```
## Compute agglomerative clustering with `agnes()`
- report the agglomerative coefficient (AC), which measures the amount of clustering structure found
  - in other words AC describes the strength of the clustering structure 
  - values closer to 1 suggest a more balanced clustering structure 
  - values closer to 0 suggest less well-formed clusters 
  - however, note that AC tends to increase as *n* increases 
- plot dendrogram 
```{r}
hc2 <- agnes(dune, method = "complete")
hc2$ac

plot(hc2, which.plot = 2)
```

## Linkage methods to assess: 
- average
- single
- complete
- ward 
- calculate the agglomerative coefficient for each linkage method 
```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

## Compute ward linkage glustering with `agnes()` 
- note that the ward method returned the highest agglomerative coefficient (AC) 
- report ward agglomerative coefficient 
- plot dendrogram 
```{r}
hc3 <- agnes(dune, method = "ward")

hc3$ac

plot(hc3, which.plot = 2)
```

## Divisive hierarchical clustering on `dune` using `diana()` 
- compute divisive coefficienct (DC), which tells us about the amount of clustering structure found
  - a DC closer to 1 suggests stronger group distinctions
- if you assess the various methods, similar to the prior agglomerative example, the ward method returns the highest DC 
```{r}
hc4 <- diana(dune)

hc4$dc
```

## Determining optimal clusters 
- dotted lines indicate optimal `k` for that method 
- there may not always be a clear optimal number of clusters 
- plot cluster results using various methods 
  - elbow method
  - silhouette method
  - gap statistic 
```{r}
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

## Working with dendrograms 
- the height of the branch between an observation and the clusters of observations below them indicate the distance between the observation and that cluster it is joined to
- for example, 15 is closer to 20 than 16 is 
- construct dendorgram for the Ames housing example
```{r}
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

To identify clusters, we can cut the dendrogram using `cutree()` 
- we can cut the agglomerative hierarchical clustering model into *k* = 4 clusters
```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```
We can now plot the full dendrogram and the 4 clusters we just created 
```{r}
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```
# Lab 2b: Ordination 

## Principal Components Analysis (PCA) 

## Data set up 
- get the data 
- the `my_basket` data set identifies items and quantities purchased for 2,000 transactions from a grocery store 
  - each observation is a single basket of goods that were purchased together 
- using this dataset, we will conduct an unsupervised analysis and try to use the attributes of each basket to identify items commonly purchased together 
```{r}
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
my_basket
```
## Start up `h2o`
- `h2o` provides consistency across dimension reduction methods 
- convert data to an `h2o` object 
```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

my_basket.h2o <- as.h2o(my_basket)
```

## Run PCA using `h2o.prcomp()` 
- use "GramSVD" method for numeric data ("GLRM" method can be used for categorical data) 
- typically, PC1 captures the most variation followed by PC2, PC3, etc. 
```{r}
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

## Plot PCA
- assess what items most contribute to PC1
  - for example, Bulmers (a brand of cider) influences PC1 the most 
- compare PC1 to PC2 to identify distinct and similar groupings of items that contribute to them 
  - for example, adult beverages contribute more to PC1 than PC2 
```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point() + 
  theme_classic()

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text() + 
  theme_classic()
```

## Eigenvalue criterion 
- the sum of eigenvalues = number of variables entered into the PCA
- eigenvalues can range from near zero to greater than 1 
- and eigenvalue of 1 means the PC explains about 1 variable's worth of the variability 
- each eigenvalue should explain at least one variable's worth of variability, so only components with eigenvalues > 1 should be retained 
  - in our case, we should only retain PC1 - PC10 
```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

## Proportion of variance explained (PVE) 
- PVE identifies the optimal number of PCs to keep 
- PVE is based on the total variability we want to account for 
- `h2o.prcomp()` provides the PVE and cumulative variance explained (CVE) 
```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```
How many PCs do we need to explain at least 75% of the total variability? 
```{r}
min(which(ve$CVE >= 0.75))
```
##Scree plot criterion 
- a scree plot shows the eigenvalues or PVE for each PC 
- scree plots often start off high and fall rather quickly because the first few PCs often explain much of the variability, the next few PCs explain a moderate amount, and the last components explain very little 
- this creates an "elbow" curve 
```{r}
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002) + 
  theme_classic()
```

## Non-metrid MultiDimensional Scaling (NMDS) 